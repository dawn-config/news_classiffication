{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "453f26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages/libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Used in POS tagging\n",
    "import re\n",
    "\n",
    "#Cleaning data packages\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import digits\n",
    "\n",
    "#Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Document-Term Matrix/TFIDF packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#For sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Classifier Packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Performance Analysis Packages\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffcf35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/userina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#clean data by eliminating punctuation, numbers, stop words, etc\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def txt_clean(txt):\n",
    "    #remove punctuation\n",
    "    txt = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    #remove digits\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    txt = txt.translate(remove_digits)  \n",
    "    \n",
    "    #create word tokens\n",
    "    tokens = re.split('\\W+', txt)\n",
    "    \n",
    "    #remove stopwords\n",
    "  #  stopword_remover = lambda y: ' '.join([word for word in y.split() if word not in (stopwords)])\n",
    "\n",
    "  #  df_column =df_column.apply(stopword_remover)    \n",
    "    \n",
    "    \n",
    "    #lemmatize words\n",
    "    txt = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(txt)\n",
    "\n",
    "def clean_data(df_column):\n",
    "    #cleaner function txt_clean  (See definition above)\n",
    "    cleaner = lambda x: txt_clean(x)\n",
    "\n",
    "    #remove stopwords\n",
    "    stopword_remover = lambda y: ' '.join([word for word in y.split() if word not in (stopwords)])\n",
    "\n",
    "    df_column =df_column.apply(stopword_remover)\n",
    "    df_column = df_column.apply(cleaner)\n",
    "\n",
    "    return df_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97de2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# especially for TFIDF matrix\n",
    "\n",
    "def make_corpus(df):\n",
    "    '''make corpus using unique document IDs\n",
    "    to ensure documents are not double counted in tfidf\n",
    "    Takes:\n",
    "        df : dataframe of training data from which to extract\n",
    "            unique documents from columns 3 and 4\n",
    "    Returns:\n",
    "        dataframe with unique documents'''\n",
    "    known = []\n",
    "    new_corpus = []\n",
    "    i = 0\n",
    "    while i < df.shape[0]:\n",
    "        ID1 = df.iloc[i, 1]\n",
    "        ID2 = df.iloc[i, 2]   \n",
    "        if ID1 not in known:\n",
    "            #add title1 text to corpus\n",
    "            new_corpus.append(df.iloc[i, 3])\n",
    "            #add ID to known\n",
    "            known.append(df.iloc[i, 1])                  \n",
    "        if ID2 not in known:\n",
    "            #add title2 text to corpus\n",
    "            new_corpus.append(df.iloc[i, 4])\n",
    "            #add ID to known\n",
    "            known.append(df.iloc[i, 2])                  \n",
    "        i+=1\n",
    "    \n",
    "    new_corpus = pd.DataFrame(new_corpus)\n",
    "    return new_corpus\n",
    "\n",
    "\n",
    "#Create TFIDF vectors for further analysis\n",
    "\n",
    "def tfidf_matrix(vectorizer, df):\n",
    "    '''Create tfidf vector for title1 and title2 columns in dataframe\n",
    "    Takes:\n",
    "        vectorizer: TfidfVectorizer() object which has been fit with training corpus\n",
    "        df: dataframe with title1 and title2 that need to be passed through tfidf\n",
    "    Returns:\n",
    "        ( t1, t2 ): tupple containing two tfidf column vectors for title1 and title2 data'''\n",
    "    t1 = vectorizer.transform(list(df.title1_en))\n",
    "    t2 = vectorizer.transform(list(df.title2_en))\n",
    "    \n",
    "    return (t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7794347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagging (have not removed upper letter, punctuation, etc.)\n",
    "\n",
    "def POS_tagging(to_tag):\n",
    "    token_vect = []\n",
    "    tagged_vect = []\n",
    "    for i in range(len(to_tag)):\n",
    "        token_vect.append(re.split('\\W+', to_tag.iloc[i][3]))\n",
    "    for j in range(len(token_vect)):\n",
    "        tagged_vect.append(nltk.pos_tag(token_vect[j]))\n",
    "    return tagged_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a7461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis to determine polarity of titles\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a1e299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity function\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    vec1 = np.array(vector1.toarray()[0])\n",
    "    vec2 = np.array(vector2.toarray()[0])\n",
    "    r = np.dot(vec1, vec2) / (np.sqrt(np.sum(vec1**2)) * np.sqrt(np.sum(vec2**2)))\n",
    "    if pd.isna(r):\n",
    "        return 0\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "def cosine_similarity_vectors(t1,t2):\n",
    "    new_X =[]\n",
    "    i = 0\n",
    "    for i in range(t1.shape[0]):\n",
    "        new_X.append(cosine_similarity(t1[i], t2[i]))\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36de6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation functions\n",
    "def validation(vectorizer,xvalidate,clf):\n",
    "    t1, t2 = tfidf_matrix(vectorizer, xvalidate)\n",
    "    validX =cosine_similarity_vectors(t1,t2)\n",
    "    x = pd.DataFrame(validX)\n",
    "    return clf.predict(x)\n",
    "\n",
    "def validationNewModel(vectorizer,xvalidate,clf):\n",
    "    t1, t2 = tfidf_matrix(vectorizer, xvalidate)\n",
    "    validX =cosine_similarity_vectors(t1,t2)\n",
    "    x = pd.DataFrame(validX)\n",
    "    x['t1_polarity'] = xvalidate.title1_en.apply(getPolarity)\n",
    "    x['t1_subjectivity'] = xvalidate.title1_en.apply(getSubjectivity)\n",
    "\n",
    "    x['t2_polarity'] = xvalidate.title2_en.apply(getPolarity)\n",
    "    x['t2_subjectivity'] = xvalidate.title2_en.apply(getSubjectivity)\n",
    "\n",
    "    x['t1_polarity'] = x['t1_polarity'].fillna(0)\n",
    "    x['t1_subjectivity'] = x['t1_subjectivity'].fillna(0)\n",
    "    x['t2_polarity'] = x['t2_polarity'].fillna(0)\n",
    "    x['t2_subjectivity'] = x['t2_subjectivity'].fillna(0)\n",
    "    return clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebbdef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "##                            D A T A   I M P O R T   \n",
    "########################################################################################## \n",
    "\n",
    "#import data and remove upper case letters as part of data cleaning\n",
    "\n",
    "train_original = pd.read_csv(\"train.csv\", dtype=str).apply(lambda x: x.astype(str).str.lower())\n",
    "test_original = pd.read_csv(\"test.csv\", dtype=str).apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "#create second copy of data to clean so it can be compared to original copy to ensure proper cleaning\n",
    "train_clean = train_original\n",
    "test_clean = test_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d640608d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256442</td>\n",
       "      <td>100672</td>\n",
       "      <td>100673</td>\n",
       "      <td>great coat brother zhu zhu wen mandarin love s...</td>\n",
       "      <td>lin xinsheng birth hard milking huo jianhua se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256443</td>\n",
       "      <td>162269</td>\n",
       "      <td>162270</td>\n",
       "      <td>nasa reveals fact ufo wreckage found moon</td>\n",
       "      <td>ufo found yuancun jiaocheng county shanxi shoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256444</td>\n",
       "      <td>157826</td>\n",
       "      <td>157854</td>\n",
       "      <td>hollow tomato loaded hormone</td>\n",
       "      <td>li chenfan bingbing home photo netizen called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256445</td>\n",
       "      <td>109579</td>\n",
       "      <td>74076</td>\n",
       "      <td>ange pavilion geoshui accurate matrimony match...</td>\n",
       "      <td>master one eightcharacter presumption marriage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256446</td>\n",
       "      <td>15068</td>\n",
       "      <td>15085</td>\n",
       "      <td>yearold busbus blow yearold child rumor rumorm...</td>\n",
       "      <td>joe johnson disgruntled timing order myth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    tid1    tid2                                          title1_en  \\\n",
       "0  256442  100672  100673  great coat brother zhu zhu wen mandarin love s...   \n",
       "1  256443  162269  162270          nasa reveals fact ufo wreckage found moon   \n",
       "2  256444  157826  157854                       hollow tomato loaded hormone   \n",
       "3  256445  109579   74076  ange pavilion geoshui accurate matrimony match...   \n",
       "4  256446   15068   15085  yearold busbus blow yearold child rumor rumorm...   \n",
       "\n",
       "                                           title2_en  \n",
       "0  lin xinsheng birth hard milking huo jianhua se...  \n",
       "1  ufo found yuancun jiaocheng county shanxi shoc...  \n",
       "2  li chenfan bingbing home photo netizen called ...  \n",
       "3  master one eightcharacter presumption marriage...  \n",
       "4         joe johnson disgruntled timing order myth   "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d393c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "##                         D A T A   C L E A N I N G\n",
    "########################################################################################## \n",
    "# Update train_clean and test_clean dataframe by removing stopwords and cleaning text \n",
    "# for columns 3 and 4 by using clean_data() which uses txt_clean()\n",
    "\n",
    "train_clean.title1_en = clean_data(train_clean.title1_en)\n",
    "train_clean.title2_en = clean_data(train_clean.title2_en)\n",
    "\n",
    "test_clean.title1_en = clean_data(test_clean.title1_en)\n",
    "test_clean.title2_en = clean_data(test_clean.title2_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cfc4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free up memory\n",
    "del train_original\n",
    "del test_original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ab725ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "##                         P R E P R O C E S S I N G\n",
    "########################################################################################## \n",
    "\n",
    "#Reserve portion of training data for validation\n",
    "#Randomly Split train_clean into train (70%) and validate (30%)\n",
    "X_train = train_clean.iloc[:, :-1]\n",
    "y_train = train_clean.iloc[:, -1]\n",
    "#X_train,X_validate,y_train,y_validate = train_test_split(X, y, test_size=0.3, train_size=0.7)\n",
    "\n",
    "#using all of the training data provided this time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcab9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus for TFIDF\n",
    "#This cell takes about 9.5 minutes to run\n",
    "new_corpus = make_corpus(train_clean)\n",
    "\n",
    "# Create vectorizer for tfidf using TfidfVectorizer and then fit the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(new_corpus[0])\n",
    "\n",
    "#Create TFIDF vectors t1 and t2 using tfidf_matrix() (see above)\n",
    "t1, t2 = tfidf_matrix(vectorizer, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49bfd4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "##                         M O D E L    T R A I N I N G\n",
    "########################################################################################## \n",
    "\n",
    "#Compute cosine similarity\n",
    "#Takes about 1.75 minutes to run\n",
    "newX = cosine_similarity_vectors(t1,t2)\n",
    "\n",
    "x = pd.DataFrame(newX)\n",
    "\n",
    "\n",
    "#Sentiment analysis to determine polarity of titles\n",
    "x['t1_polarity'] = train_clean.title1_en.apply(getPolarity)\n",
    "x['t1_subjectivity'] = train_clean.title1_en.apply(getSubjectivity)\n",
    "\n",
    "x['t2_polarity'] = train_clean.title2_en.apply(getPolarity)\n",
    "x['t2_subjectivity'] = train_clean.title2_en.apply(getSubjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47e80d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1677: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', multi_class='multinomial')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### C R E A T E   M O D E L S #####\n",
    "\n",
    "# Create and train/fit Balanced Multinomial Logistic Regression  model\n",
    "logistic_regressionMN_Balanced_clf = LogisticRegression(class_weight= 'balanced',multi_class=\"multinomial\")\n",
    "logistic_regressionMN_Balanced_clf.fit(x,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2a50355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1677: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "####### P R E D I C T I O N S    F O R    T E S T    D A T A ######\n",
    "\n",
    "y_pred_logRegMN_Balanced = validationNewModel(vectorizer,test_clean, logistic_regressionMN_Balanced_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cf46532",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(test_clean.id)\n",
    "predictions['label'] = y_pred_logRegMN_Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "747a84ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256442</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256443</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256444</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256445</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256446</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      label\n",
       "0  256442  unrelated\n",
       "1  256443  disagreed\n",
       "2  256444  unrelated\n",
       "3  256445  unrelated\n",
       "4  256446  unrelated"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "418cb489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
